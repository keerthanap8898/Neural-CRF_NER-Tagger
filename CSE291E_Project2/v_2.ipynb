{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "v_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnbGR5wpCL8"
      },
      "source": [
        "# CSE 291 Assignment 2 BiLSTM CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs2O4920pCob"
      },
      "source": [
        "## Download Data/Eval Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmfarI0hpHj6",
        "outputId": "2c9995fb-b7f3-4131-c3f0-7dcf183958e8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
        "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-30 05:08:28--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py’\n",
            "\n",
            "\rconlleval.py          0%[                    ]       0  --.-KB/s               \rconlleval.py        100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-30 05:08:29 (102 MB/s) - ‘conlleval.py’ saved [7502/7502]\n",
            "\n",
            "--2021-05-30 05:08:29--  https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 745734 (728K) [text/plain]\n",
            "Saving to: ‘train.data.quad’\n",
            "\n",
            "train.data.quad     100%[===================>] 728.26K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-05-30 05:08:29 (23.3 MB/s) - ‘train.data.quad’ saved [745734/745734]\n",
            "\n",
            "--2021-05-30 05:08:29--  https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179141 (175K) [text/plain]\n",
            "Saving to: ‘dev.data.quad’\n",
            "\n",
            "dev.data.quad       100%[===================>] 174.94K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-30 05:08:29 (10.4 MB/s) - ‘dev.data.quad’ saved [179141/179141]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CMvXrmwpNCM",
        "outputId": "a213a9f9-55f6-4029-d357-51831db64dc9"
      },
      "source": [
        "import conlleval\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "torch.manual_seed(291)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBmqHytpTGs"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKfmSZs8pPBV",
        "outputId": "d989dd41-0b7a-40ed-a606-285b61a7220a"
      },
      "source": [
        "TRAIN_DATA = 'train.data.quad'\n",
        "VALID_DATA = 'dev.data.quad'\n",
        "UNK = '<unk>'\n",
        "PAD = '<pad>'\n",
        "START_TAG = \"<start>\"  # you can add this explicitly or use it implicitly in your CRF layer\n",
        "STOP_TAG = \"<stop>\"    # you can add this explicitly or use it implicitly in your CRF layer\n",
        "\n",
        "\n",
        "def read_conll_sentence(path):\n",
        "    \"\"\" Read a CONLL-format sentence into vocab objects\n",
        "    Args:\n",
        "        :param path: path to CONLL-format data file\n",
        "        :param word_vocab: Vocabulary object for source\n",
        "        :param label_vocab: Vocabulary object for target\n",
        "    \"\"\"\n",
        "    sent = [[], []]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            if line:\n",
        "                # replace numbers with 0000\n",
        "                word = line[0]\n",
        "                word = '0000' if word.isnumeric() else word\n",
        "                sent[0].append(word)\n",
        "                sent[1].append(line[3])\n",
        "            else:\n",
        "                yield sent[0], sent[1]\n",
        "                sent = [[], []]\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset, word_vocab, label_vocab):\n",
        "    dataset = [\n",
        "      [\n",
        "        torch.tensor([word_vocab.stoi[word] for word in sent[0]], dtype=torch.long),\n",
        "        torch.tensor([label_vocab.stoi[label] for label in sent[1]], dtype=torch.long),\n",
        "      ]\n",
        "      for sent in dataset\n",
        "    ]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# load a list of sentences, where each word in the list is a tuple containing the word and the label\n",
        "train_data = list(read_conll_sentence(TRAIN_DATA))\n",
        "train_word_counter = Counter([word for sent in train_data for word in sent[0]])\n",
        "train_label_counter = Counter([label for sent in train_data for label in sent[1]])\n",
        "word_vocab = Vocab(train_word_counter, specials=(UNK, PAD), min_freq=2)\n",
        "label_vocab = Vocab(train_label_counter, specials=(), min_freq=1)\n",
        "train_data = prepare_dataset(train_data, word_vocab, label_vocab)\n",
        "d={START_TAG:8,STOP_TAG:9}\n",
        "label_vocab.stoi.update(d)\n",
        "print(label_vocab.stoi[START_TAG])\n",
        "print('Train word vocab:', len(word_vocab), 'symbols.')\n",
        "print('Train label vocab:', len(label_vocab), f'symbols: {list(label_vocab.stoi.keys())}')\n",
        "valid_data = list(read_conll_sentence(VALID_DATA))\n",
        "valid_data = prepare_dataset(valid_data, word_vocab, label_vocab)\n",
        "print('Train data:', len(train_data), 'sentences.')\n",
        "print('Valid data:', len(valid_data))\n",
        "\n",
        "print(' '.join([word_vocab.itos[i.item()] for i in train_data[0][0]]))\n",
        "print(' '.join([label_vocab.itos[i.item()] for i in train_data[0][1]]))\n",
        "\n",
        "print(' '.join([word_vocab.itos[i.item()] for i in valid_data[1][0]]))\n",
        "print(' '.join([label_vocab.itos[i.item()] for i in valid_data[1][1]]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "Train word vocab: 3947 symbols.\n",
            "Train label vocab: 8 symbols: ['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC', '<start>', '<stop>']\n",
            "Train data: 3420 sentences.\n",
            "Valid data: 800\n",
            "Pusan 0000 0000 0000 0000 0000 0000\n",
            "I-ORG O O O O O O\n",
            "Earlier this month , <unk> denied a Kabul government statement that the two sides had agreed to a ceasefire in the north .\n",
            "O O O O I-PER O O I-LOC O O O O O O O O O O O O O O O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg9R4Pn8Naxn"
      },
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNNmZx_Uqy7q"
      },
      "source": [
        "## BiLSTMTagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "a5nVIM_Eq1ZU",
        "outputId": "b04f87fe-1eee-42cd-fcdc-4179c1d7fe68"
      },
      "source": [
        "# Starter code implementing a BiLSTM Tagger\n",
        "# which makes locally normalized, independent\n",
        "# tag classifications at each time step\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, dropout=0.3):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix.keys())\n",
        "        #self.tagset_size = tag_vocab_size\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True, batch_first=True).to(device)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas.to(device)\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.dropout(self.word_embeds(sentence))\n",
        "        #embeds=embeds[0]\n",
        "        \n",
        "        #embeds=embeds.view(-1, self.tagset_size)\n",
        "        #hidden.view(-1, self.tagset_size)\n",
        "        #print(f\"embeds: {embeds}\\n word embeds: {self.word_embeds(sentence)}\\n self.hidden {self.hidden}\")\n",
        "        bilstm_out, hidden = self.bilstm(embeds, self.hidden)\n",
        "\n",
        "        bilstm_out = self.dropout(bilstm_out)\n",
        "        bilstm_out = bilstm_out\n",
        "        bilstm_feats = self.hidden2tag(bilstm_out)\n",
        "        return bilstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        cuda0=torch.device('cuda:0')\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long,device=cuda0), tags[0]])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars.to(device)\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence).view(-1, self.tagset_size).to(device)\n",
        "        forward_score = self._forward_alg(feats).to(device)\n",
        "        gold_score = self._score_sentence(feats, tags).to(device)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        bilstm_feats = self._get_lstm_features(sentence).view(-1,self.tagset_size)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(bilstm_feats)\n",
        "        return score, tag_seq\n",
        "    def loss(self, sentence, tags):##starter_modified\n",
        "        #bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
        "        r= self.neg_log_likelihood(sentence, tags)\n",
        "        # transform predictions to (n_examples, n_classes) and ground truth to (n_examples)\n",
        "        return r[0] #########################\n",
        "'''def compute_lstm_emission_features(self, sentence):##starter\n",
        "        hidden = self.init_hidden()\n",
        "        embeds = self.dropout(self.word_embeds(sentence))\n",
        "        #print(f\"embeds: {embeds}\\n word embeds: {self.word_embeds(sentence)}\\n self.hidden {hidden}\")\n",
        "        bilstm_out, hidden = self.bilstm(embeds, hidden)\n",
        "        bilstm_out = self.dropout(bilstm_out)\n",
        "        bilstm_out = bilstm_out\n",
        "        bilstm_feats = self.tag_projection_layer(bilstm_out)\n",
        "        return bilstm_feats\n",
        "def forward(self, sentence):##starter_has copy\n",
        "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
        "        return bilstm_feats.max(-1)[0].sum(), bilstm_feats.argmax(-1)'''\n",
        "    #\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def compute_lstm_emission_features(self, sentence):##starter\\n        hidden = self.init_hidden()\\n        embeds = self.dropout(self.word_embeds(sentence))\\n        #print(f\"embeds: {embeds}\\n word embeds: {self.word_embeds(sentence)}\\n self.hidden {hidden}\")\\n        bilstm_out, hidden = self.bilstm(embeds, hidden)\\n        bilstm_out = self.dropout(bilstm_out)\\n        bilstm_out = bilstm_out\\n        bilstm_feats = self.tag_projection_layer(bilstm_out)\\n        return bilstm_feats\\ndef forward(self, sentence):##starter_has copy\\n        bilstm_feats = self.compute_lstm_emission_features(sentence)\\n        return bilstm_feats.max(-1)[0].sum(), bilstm_feats.argmax(-1)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH7JGSDAruUg"
      },
      "source": [
        "## Train / Eval loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw2He2cgrrF1"
      },
      "source": [
        "def train(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
        "    losses_per_epoch = []\n",
        "    F1_scores=[]\n",
        "    lossavg=[]\n",
        "    valid_loss=[]\n",
        "    for epoch in range(epochs):\n",
        "        print(f'--- EPOCH {epoch} ---')\n",
        "        model.train()\n",
        "        losses_per_epoch.append([])\n",
        "        for i, (sent, tags) in enumerate(train_data):\n",
        "            model.zero_grad()\n",
        "            sent, tags = sent.to(device), tags.to(device)\n",
        "            sent = sent.unsqueeze(0)\n",
        "            tags = tags.unsqueeze(0)\n",
        "            loss = model.loss(sent, tags)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
        "            if i > 0 and i % log_interval == 0:\n",
        "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
        "\n",
        "        k, li=evaluate(model, valid_data, word_vocab, label_vocab)\n",
        "        F1_scores.append(k)\n",
        "        print(F1_scores)\n",
        "        sum=0\n",
        "        for i in losses_per_epoch[-1]:\n",
        "            sum+=i\n",
        "        sum=sum/len(losses_per_epoch[-1])\n",
        "        lossavg.append(sum)\n",
        "        print(lossavg)\n",
        "        valid_loss.append(li)\n",
        "    return F1_scores, lossavg, valid_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, word_vocab, label_vocab):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    scores = []\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "    sents = []\n",
        "    for i, (sent, tags) in enumerate(dataset):\n",
        "        with torch.no_grad():\n",
        "            sent, tags = sent.to(device), tags.to(device)\n",
        "            sent = sent.unsqueeze(0)\n",
        "            tags = tags.unsqueeze(0)\n",
        "            \n",
        "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
        "            score, pred_tag_seq = model(sent)\n",
        "            scores.append(score.cpu().detach().numpy())\n",
        "            true_tags.append([label_vocab.itos[i] for i in tags.tolist()[0]])\n",
        "            #print(pred_tag_seq)\n",
        "            pred_tags.append([label_vocab.itos[i] for i in pred_tag_seq])\n",
        "            sents.append([word_vocab.itos[i] for i in sent[0]])\n",
        "\n",
        "    print('Avg evaluation loss:', np.mean(losses))\n",
        "    F1=conlleval.evaluate([tag for tags in true_tags for tag in tags], [tag for tags in pred_tags for tag in tags], verbose=True)\n",
        "    print(F1)\n",
        "    print('\\n5 random evaluation samples:')\n",
        "    for i in np.random.randint(0, len(sents), size=5):\n",
        "        print('SENT:', ' '.join(sents[i]))\n",
        "        print('TRUE:', ' '.join(true_tags[i]))\n",
        "        print('PRED:', ' '.join(pred_tags[i]))\n",
        "    return F1[2], np.mean(losses)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdJsc_y6rxdC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyfoJfZry4-",
        "outputId": "737c0e3f-086b-463d-c452-1c3cba60348f"
      },
      "source": [
        "# Train BiLSTM Tagger Baseline\n",
        "model = BiLSTM_CRF(len(word_vocab), label_vocab.stoi, 128, 256).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "F1_scores, losses, l=train(model, train_data, valid_data, word_vocab, label_vocab, epochs=20, log_interval=500)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- EPOCH 0 ---\n",
            "Avg loss over last 500 updates: 10.640667314529418\n",
            "Avg loss over last 500 updates: 8.562806468963624\n",
            "Avg loss over last 500 updates: 6.6640385255813595\n",
            "Avg loss over last 500 updates: 5.836283703804016\n",
            "Avg loss over last 500 updates: 4.939681134223938\n",
            "Avg loss over last 500 updates: 4.945787421226502\n",
            "Avg evaluation loss: 4.526355362534523\n",
            "processed 11170 tokens with 1231 phrases; found: 731 phrases; correct: 452.\n",
            "accuracy:  44.11%; (non-O)\n",
            "accuracy:  88.74%; precision:  61.83%; recall:  36.72%; FB1:  46.08\n",
            "              LOC: precision:  77.53%; recall:  38.02%; FB1:  51.02  178\n",
            "             MISC: precision:  68.57%; recall:  12.50%; FB1:  21.15  35\n",
            "              ORG: precision:  62.23%; recall:  38.11%; FB1:  47.27  188\n",
            "              PER: precision:  52.42%; recall:  46.88%; FB1:  49.50  330\n",
            "(61.83310533515732, 36.718115353371246, 46.07543323139653)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: CALIFORNIA 0000 0000 <unk> 0000\n",
            "TRUE: I-ORG O O O O\n",
            "PRED: I-ORG O O I-ORG O\n",
            "SENT: Austrian television reported earlier that more than 0000 had been hurt in the accident at the <unk> in <unk> , 0000 km ( 0000 miles ) west of Vienna .\n",
            "TRUE: I-MISC O O O O O O O O O O O O O O O O O I-LOC O O O O O O O O O I-LOC O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "SENT: <unk> had been working for the <unk> which provides food to civilians for only a few weeks before he was <unk> .\n",
            "TRUE: I-PER O O O O O O O O O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O\n",
            "SENT: Poland 's Foreign Minister <unk> Rosati will visit Yugoslavia on September 0000 and 0000 to revive a <unk> between the two governments which was <unk> <unk> in 0000 , <unk> news agency reported on Friday .\n",
            "TRUE: I-LOC O O O I-PER I-PER O O I-LOC O O O O O O O O O O O O O O O O O O O O I-ORG O O O O O O\n",
            "PRED: I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "SENT: \" The market was <unk> quiet today , some <unk> <unk> , no Western orders , \" said Nick <unk> , director of sales and trade at <unk> . \"\n",
            "TRUE: O O O O O O O O O O O O O I-MISC O O O O I-PER I-PER O O O O O O O I-ORG O O\n",
            "PRED: O O O O O O O O O I-PER I-PER O O O O O O O I-PER I-PER O O O O O O O O O O\n",
            "[46.07543323139653]\n",
            "[6.6432928626997425]\n",
            "--- EPOCH 1 ---\n",
            "Avg loss over last 500 updates: 4.211031124591828\n",
            "Avg loss over last 500 updates: 4.482707698822021\n",
            "Avg loss over last 500 updates: 3.7437935633659363\n",
            "Avg loss over last 500 updates: 3.573149621486664\n",
            "Avg loss over last 500 updates: 3.062616895198822\n",
            "Avg loss over last 500 updates: 3.139836820602417\n",
            "Avg evaluation loss: 3.409770675301552\n",
            "processed 11170 tokens with 1231 phrases; found: 898 phrases; correct: 628.\n",
            "accuracy:  59.14%; (non-O)\n",
            "accuracy:  91.14%; precision:  69.93%; recall:  51.02%; FB1:  58.99\n",
            "              LOC: precision:  84.12%; recall:  53.99%; FB1:  65.77  233\n",
            "             MISC: precision:  75.53%; recall:  36.98%; FB1:  49.65  94\n",
            "              ORG: precision:  63.16%; recall:  46.91%; FB1:  53.83  228\n",
            "              PER: precision:  63.27%; recall:  58.81%; FB1:  60.96  343\n",
            "(69.93318485523386, 51.015434606011375, 58.994833255049315)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: ( Chicago newsdesk 312-408-8720 )\n",
            "TRUE: O I-LOC O O O\n",
            "PRED: O I-LOC O O O\n",
            "SENT: <unk>\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: TENNIS - <unk> <unk> <unk> - <unk> <unk> <unk> <unk> <unk> AT U.S. OPEN .\n",
            "TRUE: O O O O O O O O O O O O I-MISC I-MISC O\n",
            "PRED: O O O O O O I-ORG I-ORG I-ORG I-ORG I-ORG O I-MISC I-MISC O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: Spanish police will <unk> fans at the <unk> of the <unk> <unk> stadium and ban <unk> supporters from Saturday 's big Real <unk> game , the Madrid daily El <unk> said on Friday .\n",
            "TRUE: I-MISC O O O O O O O O O I-LOC I-LOC I-LOC O O O O O O O O I-MISC I-MISC O O O I-LOC O I-ORG I-ORG O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O O O O O I-ORG I-ORG I-ORG I-ORG O O O O\n",
            "[46.07543323139653, 58.994833255049315]\n",
            "[6.6432928626997425, 3.624143900020778]\n",
            "--- EPOCH 2 ---\n",
            "Avg loss over last 500 updates: 2.868334907531738\n",
            "Avg loss over last 500 updates: 3.1224423933029173\n",
            "Avg loss over last 500 updates: 2.624223864078522\n",
            "Avg loss over last 500 updates: 2.6680086398124696\n",
            "Avg loss over last 500 updates: 2.246499466896057\n",
            "Avg loss over last 500 updates: 2.40092244720459\n",
            "Avg evaluation loss: 2.974259833395481\n",
            "processed 11170 tokens with 1231 phrases; found: 965 phrases; correct: 722.\n",
            "accuracy:  65.09%; (non-O)\n",
            "accuracy:  92.68%; precision:  74.82%; recall:  58.65%; FB1:  65.76\n",
            "              LOC: precision:  85.92%; recall:  67.22%; FB1:  75.43  284\n",
            "             MISC: precision:  81.90%; recall:  44.79%; FB1:  57.91  105\n",
            "              ORG: precision:  65.53%; recall:  50.16%; FB1:  56.83  235\n",
            "              PER: precision:  69.79%; recall:  64.50%; FB1:  67.04  341\n",
            "(74.81865284974093, 58.651502843216896, 65.75591985428049)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: CHICAGO 1996-08-26\n",
            "TRUE: I-LOC O\n",
            "PRED: I-LOC O\n",
            "SENT: <unk> 0000 <unk> 0000\n",
            "TRUE: I-ORG O I-ORG O\n",
            "PRED: I-ORG O I-ORG O\n",
            "SENT: AMSTERDAM 1996-08-29\n",
            "TRUE: I-LOC O\n",
            "PRED: I-LOC O\n",
            "SENT: It said Craig <unk> has also joined the company as executive <unk> president and chief <unk> officer .\n",
            "TRUE: O O I-PER I-PER O O O O O O O O O O O O O O\n",
            "PRED: O O I-PER I-PER O O O O O O O O O O O O O O\n",
            "SENT: The Brazilian 's <unk> effort was enough to <unk> Real a point from a <unk> 1-1 draw at fellow title contenders <unk> <unk> .\n",
            "TRUE: O I-MISC O O O O O O O I-ORG O O O O O O O O O O O I-ORG I-ORG O\n",
            "PRED: O I-PER O O O O O O O O O O O O O O O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195]\n",
            "--- EPOCH 3 ---\n",
            "Avg loss over last 500 updates: 2.1717420997619628\n",
            "Avg loss over last 500 updates: 2.485507881164551\n",
            "Avg loss over last 500 updates: 2.025351625442505\n",
            "Avg loss over last 500 updates: 2.1852126982212066\n",
            "Avg loss over last 500 updates: 1.779721022605896\n",
            "Avg loss over last 500 updates: 1.9330990400314332\n",
            "Avg evaluation loss: 2.811852522492409\n",
            "processed 11170 tokens with 1231 phrases; found: 990 phrases; correct: 738.\n",
            "accuracy:  66.85%; (non-O)\n",
            "accuracy:  93.02%; precision:  74.55%; recall:  59.95%; FB1:  66.46\n",
            "              LOC: precision:  84.30%; recall:  68.04%; FB1:  75.30  293\n",
            "             MISC: precision:  81.60%; recall:  53.12%; FB1:  64.35  125\n",
            "              ORG: precision:  68.20%; recall:  48.21%; FB1:  56.49  217\n",
            "              PER: precision:  67.89%; recall:  65.31%; FB1:  66.57  355\n",
            "(74.54545454545455, 59.95125913891145, 66.4565511031067)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> - <unk> <unk> ( <unk> ) , <unk> <unk>\n",
            "TRUE: I-ORG O I-PER I-PER O O O O I-PER I-PER\n",
            "PRED: O O I-PER I-PER O I-LOC O O O O\n",
            "SENT: Gujral said India had national security <unk> that made it <unk> for New <unk> to sign the <unk> .\n",
            "TRUE: I-PER O I-LOC O O O O O O O O O I-LOC I-LOC O O O I-MISC O\n",
            "PRED: I-PER O I-LOC O O O O O O O O O I-LOC O O O O O O\n",
            "SENT: CRICKET - PAKISTAN <unk> AT <unk> <unk> <unk> <unk> THIRD <unk> .\n",
            "TRUE: O O I-LOC O O O O O O O O O\n",
            "PRED: O O I-ORG I-ORG O O O O O O O O\n",
            "SENT: <unk> year , 0000 new <unk> <unk> <unk> in Brazil and medical students from all over the world come to study there .\n",
            "TRUE: O O O O O O O O O I-LOC O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O I-LOC O O O O O O O O O O O O O\n",
            "SENT: \" He 's a <unk> and a <unk> , but he <unk> the <unk> between a <unk> <unk> and an <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O O I-MISC O O O I-MISC O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084]\n",
            "--- EPOCH 4 ---\n",
            "Avg loss over last 500 updates: 1.8120646288394928\n",
            "Avg loss over last 500 updates: 2.030528634548187\n",
            "Avg loss over last 500 updates: 1.7084992728233337\n",
            "Avg loss over last 500 updates: 1.8274407901763916\n",
            "Avg loss over last 500 updates: 1.5096489720344544\n",
            "Avg loss over last 500 updates: 1.649433530330658\n",
            "Avg evaluation loss: 2.7391921460628508\n",
            "processed 11170 tokens with 1231 phrases; found: 1030 phrases; correct: 784.\n",
            "accuracy:  69.71%; (non-O)\n",
            "accuracy:  93.48%; precision:  76.12%; recall:  63.69%; FB1:  69.35\n",
            "              LOC: precision:  84.66%; recall:  73.00%; FB1:  78.40  313\n",
            "             MISC: precision:  82.40%; recall:  53.65%; FB1:  64.98  125\n",
            "              ORG: precision:  65.02%; recall:  55.70%; FB1:  60.00  263\n",
            "              PER: precision:  74.47%; recall:  66.40%; FB1:  70.20  329\n",
            "(76.11650485436893, 63.6880584890333, 69.3498452012384)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: SEATTLE 0000 0000 <unk> 0000\n",
            "TRUE: I-ORG O O O O\n",
            "PRED: I-ORG O O O O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: <unk> 0000 COLORADO 0000\n",
            "TRUE: I-ORG O I-ORG O\n",
            "PRED: I-ORG O I-ORG O\n",
            "SENT: The victory against Japan <unk> the Fed Cup debut of Monica Seles , who <unk> a <unk> U.S. citizen in 0000 .\n",
            "TRUE: O O O I-LOC O O I-MISC I-MISC O O I-PER I-PER O O O O O I-LOC O O O O\n",
            "PRED: O O O I-LOC O O I-MISC I-MISC O O I-PER I-PER O O O O O I-LOC O O O O\n",
            "SENT: 9. <unk> 0000\n",
            "TRUE: O I-PER O\n",
            "PRED: O I-ORG O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808]\n",
            "--- EPOCH 5 ---\n",
            "Avg loss over last 500 updates: 1.6228844351768494\n",
            "Avg loss over last 500 updates: 1.6927629480361939\n",
            "Avg loss over last 500 updates: 1.4487774538993836\n",
            "Avg loss over last 500 updates: 1.6020061197280884\n",
            "Avg loss over last 500 updates: 1.250015028476715\n",
            "Avg loss over last 500 updates: 1.4987449884414672\n",
            "Avg evaluation loss: 2.7203063863515853\n",
            "processed 11170 tokens with 1231 phrases; found: 1049 phrases; correct: 783.\n",
            "accuracy:  69.82%; (non-O)\n",
            "accuracy:  93.53%; precision:  74.64%; recall:  63.61%; FB1:  68.68\n",
            "              LOC: precision:  84.16%; recall:  74.66%; FB1:  79.12  322\n",
            "             MISC: precision:  75.00%; recall:  50.00%; FB1:  60.00  128\n",
            "              ORG: precision:  68.22%; recall:  52.44%; FB1:  59.30  236\n",
            "              PER: precision:  70.25%; recall:  69.11%; FB1:  69.67  363\n",
            "(74.64251668255481, 63.6068237205524, 68.6842105263158)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: State media quoted China 's top <unk> with <unk> , <unk> <unk> , as <unk> a visiting group from Taiwan on Wednesday that it was time for the rivals to hold political talks .\n",
            "TRUE: O O O I-LOC O O O O I-LOC O I-PER I-PER O O O O O O O I-LOC O O O O O O O O O O O O O O\n",
            "PRED: I-LOC O O I-LOC O O O O I-PER O I-PER I-PER O O O O O O O I-LOC O O O O O O O O O O O O O O\n",
            "SENT: More <unk> results are expected soon .\n",
            "TRUE: O O O O O O O\n",
            "PRED: O O O O O O O\n",
            "SENT: <unk> Real Madrid <unk>\n",
            "TRUE: O I-ORG I-ORG O\n",
            "PRED: I-ORG I-ORG I-ORG I-ORG\n",
            "SENT: 9. Britain I ( <unk> <unk> , <unk> <unk> ) <unk> ( <unk>\n",
            "TRUE: O I-ORG I-ORG O I-PER I-PER O I-PER I-PER O O O O\n",
            "PRED: O I-LOC O O O O O I-PER I-PER O O O O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726]\n",
            "--- EPOCH 6 ---\n",
            "Avg loss over last 500 updates: 1.362214161157608\n",
            "Avg loss over last 500 updates: 1.5664786322116853\n",
            "Avg loss over last 500 updates: 1.289532814502716\n",
            "Avg loss over last 500 updates: 1.4404114542007447\n",
            "Avg loss over last 500 updates: 1.1752387795448302\n",
            "Avg loss over last 500 updates: 1.339218433380127\n",
            "Avg evaluation loss: 2.727693136036396\n",
            "processed 11170 tokens with 1231 phrases; found: 1056 phrases; correct: 808.\n",
            "accuracy:  72.03%; (non-O)\n",
            "accuracy:  93.89%; precision:  76.52%; recall:  65.64%; FB1:  70.66\n",
            "              LOC: precision:  89.35%; recall:  76.31%; FB1:  82.32  310\n",
            "             MISC: precision:  75.94%; recall:  52.60%; FB1:  62.15  133\n",
            "              ORG: precision:  66.80%; recall:  56.35%; FB1:  61.13  259\n",
            "              PER: precision:  72.60%; recall:  69.65%; FB1:  71.09  354\n",
            "(76.51515151515152, 65.63769293257515, 70.66025360734587)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> 0000 COLORADO 0000\n",
            "TRUE: I-ORG O I-ORG O\n",
            "PRED: I-ORG O I-ORG O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: <unk> , Bosnia 1996-08-25\n",
            "TRUE: I-LOC O I-LOC O\n",
            "PRED: I-LOC O I-LOC O\n",
            "SENT: <unk> to the <unk> 's weakness , another dealer said banks who might have sold <unk> in Monday 's trading appeared to <unk> away from the market .\n",
            "TRUE: O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728]\n",
            "--- EPOCH 7 ---\n",
            "Avg loss over last 500 updates: 1.1889683327674865\n",
            "Avg loss over last 500 updates: 1.3727010567188263\n",
            "Avg loss over last 500 updates: 1.238724371433258\n",
            "Avg loss over last 500 updates: 1.2082629284858704\n",
            "Avg loss over last 500 updates: 1.0124771299362183\n",
            "Avg loss over last 500 updates: 1.2171779453754425\n",
            "Avg evaluation loss: 2.7438551032543184\n",
            "processed 11170 tokens with 1231 phrases; found: 1056 phrases; correct: 830.\n",
            "accuracy:  72.69%; (non-O)\n",
            "accuracy:  94.19%; precision:  78.60%; recall:  67.42%; FB1:  72.58\n",
            "              LOC: precision:  88.40%; recall:  77.69%; FB1:  82.70  319\n",
            "             MISC: precision:  80.77%; recall:  54.69%; FB1:  65.22  130\n",
            "              ORG: precision:  69.35%; recall:  58.96%; FB1:  63.73  261\n",
            "              PER: precision:  75.72%; recall:  71.00%; FB1:  73.29  346\n",
            "(78.59848484848484, 67.42485783915517, 72.5841714035855)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: He <unk> for a <unk> and a <unk> opening round that left him five shots off the <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O\n",
            "SENT: The cars will be <unk> <unk> by <unk> and <unk> <unk> <unk> <unk> , <unk> by a son of President <unk> , which plans next year to start <unk> the <unk> in Indonesia .\n",
            "TRUE: O O O O O O O I-ORG O I-ORG I-ORG I-ORG I-ORG O O O O O O O I-PER O O O O O O O O O O O I-LOC O\n",
            "PRED: O O O O O O O O O O O I-PER I-PER O O O O O O O O O O O O O O O O O O O I-LOC O\n",
            "SENT: <unk> <unk> Bosnia is scheduled for September 0000 , when citizens are <unk> to elect <unk> and <unk> <unk> , <unk> <unk> and Serb <unk> , a national House of Representatives and a <unk> <unk> .\n",
            "TRUE: O O I-LOC O O O O O O O O O O O O O O O O O O I-MISC O I-MISC O O O O I-ORG I-ORG I-ORG O O O O O\n",
            "PRED: O O I-LOC O O O O O O O O O O O O O O I-ORG I-ORG O I-PER I-PER O I-MISC I-PER O O O O O O O O O O O\n",
            "SENT: Fall of wickets : <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "TRUE: O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804]\n",
            "--- EPOCH 8 ---\n",
            "Avg loss over last 500 updates: 1.0258596472740173\n",
            "Avg loss over last 500 updates: 1.2332261271476745\n",
            "Avg loss over last 500 updates: 1.0805347213745118\n",
            "Avg loss over last 500 updates: 1.2419098777770996\n",
            "Avg loss over last 500 updates: 0.9859331502914429\n",
            "Avg loss over last 500 updates: 1.0744613890647887\n",
            "Avg evaluation loss: 2.7880976888537408\n",
            "processed 11170 tokens with 1231 phrases; found: 1069 phrases; correct: 829.\n",
            "accuracy:  72.74%; (non-O)\n",
            "accuracy:  94.23%; precision:  77.55%; recall:  67.34%; FB1:  72.09\n",
            "              LOC: precision:  86.02%; recall:  77.96%; FB1:  81.79  329\n",
            "             MISC: precision:  82.03%; recall:  54.69%; FB1:  65.62  128\n",
            "              ORG: precision:  68.56%; recall:  58.96%; FB1:  63.40  264\n",
            "              PER: precision:  74.71%; recall:  70.46%; FB1:  72.52  348\n",
            "(77.5491113189897, 67.34362307067425, 72.08695652173913)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> <unk> futures ended <unk> but off the day 's <unk> on Thursday .\n",
            "TRUE: I-MISC I-MISC O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O\n",
            "SENT: SOCCER - <unk> <unk> <unk> <unk> .\n",
            "TRUE: O O I-ORG O O I-PER O\n",
            "PRED: O O O O O O O\n",
            "SENT: 8. <unk> <unk> ( Belgium ) <unk>\n",
            "TRUE: O I-PER I-PER O I-LOC O O\n",
            "PRED: O I-PER I-PER O I-LOC O O\n",
            "SENT: Hapoel <unk> 0000 Maccabi Tel Aviv 0000\n",
            "TRUE: I-ORG I-ORG O O I-ORG I-ORG O\n",
            "PRED: I-ORG I-ORG O I-ORG I-ORG I-ORG O\n",
            "SENT: France on Friday expelled another African man seized in a police <unk> on a Paris church as about 0000 Air France workers <unk> \" <unk> of <unk> \" used to fly illegal <unk> home .\n",
            "TRUE: I-LOC O O O O I-MISC O O O O O O O O I-LOC O O O O I-ORG I-ORG O O O O O O O O O O O O O O\n",
            "PRED: I-LOC O O O O I-MISC O O O O O O O O I-LOC O O O O O I-LOC O O O O O I-MISC O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637]\n",
            "--- EPOCH 9 ---\n",
            "Avg loss over last 500 updates: 1.0584155201911927\n",
            "Avg loss over last 500 updates: 1.0893077392578125\n",
            "Avg loss over last 500 updates: 0.9439591188430786\n",
            "Avg loss over last 500 updates: 0.9603826367855072\n",
            "Avg loss over last 500 updates: 0.883072512626648\n",
            "Avg loss over last 500 updates: 0.9728056683540344\n",
            "Avg evaluation loss: 2.8332940067350862\n",
            "processed 11170 tokens with 1231 phrases; found: 1129 phrases; correct: 842.\n",
            "accuracy:  75.00%; (non-O)\n",
            "accuracy:  93.94%; precision:  74.58%; recall:  68.40%; FB1:  71.36\n",
            "              LOC: precision:  89.56%; recall:  77.96%; FB1:  83.36  316\n",
            "             MISC: precision:  77.78%; recall:  54.69%; FB1:  64.22  135\n",
            "              ORG: precision:  66.55%; recall:  60.26%; FB1:  63.25  278\n",
            "              PER: precision:  67.25%; recall:  72.90%; FB1:  69.96  400\n",
            "(74.5792736935341, 68.39967506092609, 71.35593220338983)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> were <unk> in the money market but <unk> in bonds .\n",
            "TRUE: O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O\n",
            "SENT: <unk> details first-half operating profits .\n",
            "TRUE: I-ORG O O O O O\n",
            "PRED: O O O O O O\n",
            "SENT: CRICKET - SRI LANKA BEAT <unk> <unk> 0000 <unk> IN ONE-DAY MATCH .\n",
            "TRUE: O O I-LOC I-LOC O I-LOC O O O O O O O\n",
            "PRED: O O I-LOC I-LOC O I-LOC O O O O O O O\n",
            "SENT: <unk> Gold Inc was up C$ <unk> to C$ <unk> in trading of <unk> shares , while <unk> Gold Corp gained C$ <unk> to C$ 0000 in volume of <unk> shares .\n",
            "TRUE: I-ORG I-ORG I-ORG O O I-MISC O O I-MISC O O O O O O O O I-ORG I-ORG I-ORG O I-MISC O O I-MISC O O O O O O O\n",
            "PRED: I-ORG I-ORG I-ORG O O I-MISC O O I-MISC O O O O O O O O I-ORG I-ORG I-ORG O I-MISC O O I-MISC O O O O O O O\n",
            "SENT: One officer and a <unk> killed in <unk> clash between two groups of soldiers near <unk> in <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O I-LOC O I-LOC O\n",
            "PRED: O O O O O O O O O O O O O O O O O I-LOC O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401]\n",
            "--- EPOCH 10 ---\n",
            "Avg loss over last 500 updates: 0.9481219201087951\n",
            "Avg loss over last 500 updates: 1.1222404446601868\n",
            "Avg loss over last 500 updates: 0.8902166433334351\n",
            "Avg loss over last 500 updates: 0.920248025894165\n",
            "Avg loss over last 500 updates: 0.7713436198234558\n",
            "Avg loss over last 500 updates: 0.8996371774673462\n",
            "Avg evaluation loss: 3.0228522665798665\n",
            "processed 11170 tokens with 1231 phrases; found: 1107 phrases; correct: 849.\n",
            "accuracy:  74.83%; (non-O)\n",
            "accuracy:  94.28%; precision:  76.69%; recall:  68.97%; FB1:  72.63\n",
            "              LOC: precision:  86.10%; recall:  78.51%; FB1:  82.13  331\n",
            "             MISC: precision:  80.00%; recall:  56.25%; FB1:  66.06  135\n",
            "              ORG: precision:  73.98%; recall:  59.28%; FB1:  65.82  246\n",
            "              PER: precision:  69.37%; recall:  74.25%; FB1:  71.73  395\n",
            "(76.69376693766937, 68.96831844029244, 72.6261762189906)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: FOR <unk> <unk> <unk> <unk>\n",
            "TRUE: O I-MISC I-MISC O O\n",
            "PRED: O O O O O\n",
            "SENT: <unk> <unk> <unk> Grand Prix on Sunday :\n",
            "TRUE: I-MISC O O I-MISC I-MISC O O O\n",
            "PRED: I-MISC I-MISC I-MISC I-MISC I-MISC O O O\n",
            "SENT: <unk> only hours after Chinese state media said the time was right to <unk> in political talks with Taiwan , Foreign Ministry spokesman <unk> <unk> told Reuters : \" The <unk> <unk> for the opening of the talks has been <unk> by the Taiwan authorities . \"\n",
            "TRUE: O O O O I-MISC O O O O O O O O O O O O O I-LOC O I-ORG I-ORG O I-PER I-PER O I-ORG O O O O O O O O O O O O O O O O I-LOC O O O\n",
            "PRED: O O O O I-MISC O O O O O O O O O O O O O I-LOC O I-ORG I-ORG O I-PER I-PER O I-ORG O O O O O O O O O O O O O O O O I-LOC O O O\n",
            "SENT: In an interview following its first-half results , which included a less <unk> forecast for the second half of this year than it had made in the past , Sir Colin <unk> said <unk> had taken defensive action to <unk> it from <unk> markets .\n",
            "TRUE: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O I-PER I-PER O I-ORG O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O I-PER O O O O O O O O O O O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313]\n",
            "--- EPOCH 11 ---\n",
            "Avg loss over last 500 updates: 0.874133173942566\n",
            "Avg loss over last 500 updates: 1.0493294432163238\n",
            "Avg loss over last 500 updates: 0.9039114165306091\n",
            "Avg loss over last 500 updates: 0.8879976530075073\n",
            "Avg loss over last 500 updates: 0.7772748022079468\n",
            "Avg loss over last 500 updates: 0.8743943500518799\n",
            "Avg evaluation loss: 3.168568170815706\n",
            "processed 11170 tokens with 1231 phrases; found: 1081 phrases; correct: 837.\n",
            "accuracy:  72.41%; (non-O)\n",
            "accuracy:  94.15%; precision:  77.43%; recall:  67.99%; FB1:  72.40\n",
            "              LOC: precision:  88.27%; recall:  78.79%; FB1:  83.26  324\n",
            "             MISC: precision:  80.15%; recall:  56.77%; FB1:  66.46  136\n",
            "              ORG: precision:  73.48%; recall:  55.05%; FB1:  62.94  230\n",
            "              PER: precision:  69.82%; recall:  73.98%; FB1:  71.84  391\n",
            "(77.42830712303423, 67.99350121852153, 72.40484429065742)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: CALIFORNIA 0000 0000 <unk> 0000\n",
            "TRUE: I-ORG O O O O\n",
            "PRED: I-ORG O O O O\n",
            "SENT: <unk> Munich 0000 0000 0000 0000 0000 0000 0000\n",
            "TRUE: I-ORG I-ORG O O O O O O O\n",
            "PRED: I-ORG I-ORG O O O O O O O\n",
            "SENT: <unk> <unk> 0000 0000 0000 0000 0000 0000 0000\n",
            "TRUE: I-ORG I-ORG O O O O O O O\n",
            "PRED: I-ORG I-ORG O O O O O O O\n",
            "SENT: <unk> <unk> futures ended <unk> but off the day 's <unk> on Thursday .\n",
            "TRUE: I-MISC I-MISC O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O\n",
            "SENT: \" My <unk> this year has been <unk> , \" <unk> said . \"\n",
            "TRUE: O O O O O O O O O O I-PER O O O\n",
            "PRED: O O O O O O O O O O I-PER O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511]\n",
            "--- EPOCH 12 ---\n",
            "Avg loss over last 500 updates: 0.795053291797638\n",
            "Avg loss over last 500 updates: 0.9314268567562103\n",
            "Avg loss over last 500 updates: 0.8642476558685303\n",
            "Avg loss over last 500 updates: 0.8594230945110322\n",
            "Avg loss over last 500 updates: 0.7134820079803467\n",
            "Avg loss over last 500 updates: 0.8068802521228791\n",
            "Avg evaluation loss: 3.069324896633625\n",
            "processed 11170 tokens with 1231 phrases; found: 1119 phrases; correct: 853.\n",
            "accuracy:  74.94%; (non-O)\n",
            "accuracy:  94.37%; precision:  76.23%; recall:  69.29%; FB1:  72.60\n",
            "              LOC: precision:  91.78%; recall:  76.86%; FB1:  83.66  304\n",
            "             MISC: precision:  78.01%; recall:  57.29%; FB1:  66.07  141\n",
            "              ORG: precision:  70.04%; recall:  60.91%; FB1:  65.16  267\n",
            "              PER: precision:  68.06%; recall:  75.07%; FB1:  71.39  407\n",
            "(76.22877569258266, 69.2932575142161, 72.59574468085106)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: U.S. <unk> <unk> fire while landing in Israel .\n",
            "TRUE: I-LOC I-MISC O O O O O I-LOC O\n",
            "PRED: I-LOC O O O O O O I-LOC O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: England <unk> their championship <unk> when they decided to sign a <unk> million pounds sterling ( $ <unk> million ) deal giving <unk> television <unk> rights to <unk> union matches in England .\n",
            "TRUE: I-LOC O O O O O O O O O O O O O O O O O O O O O I-ORG O O O O O O O O I-LOC O\n",
            "PRED: I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O I-LOC O\n",
            "SENT: <unk>\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: Johnson said that <unk> interest rates have already been <unk> by the election .\n",
            "TRUE: I-PER O O O O O O O O O O O O O\n",
            "PRED: I-PER O O O O O O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392]\n",
            "--- EPOCH 13 ---\n",
            "Avg loss over last 500 updates: 0.8249434134960174\n",
            "Avg loss over last 500 updates: 1.0031816115379333\n",
            "Avg loss over last 500 updates: 0.7303474779129029\n",
            "Avg loss over last 500 updates: 0.7209996275901794\n",
            "Avg loss over last 500 updates: 0.6952915215492248\n",
            "Avg loss over last 500 updates: 0.738655092716217\n",
            "Avg evaluation loss: 3.238306036889553\n",
            "processed 11170 tokens with 1231 phrases; found: 1109 phrases; correct: 850.\n",
            "accuracy:  74.34%; (non-O)\n",
            "accuracy:  94.32%; precision:  76.65%; recall:  69.05%; FB1:  72.65\n",
            "              LOC: precision:  89.16%; recall:  79.34%; FB1:  83.97  323\n",
            "             MISC: precision:  81.62%; recall:  57.81%; FB1:  67.68  136\n",
            "              ORG: precision:  73.17%; recall:  58.63%; FB1:  65.10  246\n",
            "              PER: precision:  67.08%; recall:  73.44%; FB1:  70.12  404\n",
            "(76.64562669071235, 69.04955320877335, 72.64957264957265)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> <unk> --- --- --- --- <unk> up <unk>\n",
            "TRUE: I-LOC B-LOC O O O O O O O\n",
            "PRED: O O O O O O O O O\n",
            "SENT: <unk> 0000 0000 0000 0000 0000 0000 0000\n",
            "TRUE: I-ORG O O O O O O O\n",
            "PRED: I-ORG O O O O O O O\n",
            "SENT: <unk> 1996-08-26\n",
            "TRUE: I-LOC O\n",
            "PRED: I-LOC O\n",
            "SENT: An <unk> of <unk> has killed five people in the central Senegal town of <unk> , where health authorities have <unk> 0000 cases since August 0000 , a medical official said on Thursday .\n",
            "TRUE: O O O O O O O O O O O I-LOC O O I-LOC O O O O O O O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O I-LOC O O I-LOC O O O O O O O O O O O O O O O O O O O\n",
            "SENT: <unk> said fourth quarter earnings and revenues were expected to fall short of <unk> .\n",
            "TRUE: I-ORG O O O O O O O O O O O O O O\n",
            "PRED: I-PER O O O O O O O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126]\n",
            "--- EPOCH 14 ---\n",
            "Avg loss over last 500 updates: 0.705255733013153\n",
            "Avg loss over last 500 updates: 0.8048647768497467\n",
            "Avg loss over last 500 updates: 0.7993097114562988\n",
            "Avg loss over last 500 updates: 0.7862960920333862\n",
            "Avg loss over last 500 updates: 0.5678378164768219\n",
            "Avg loss over last 500 updates: 0.7326659436225891\n",
            "Avg evaluation loss: 3.282989797592163\n",
            "processed 11170 tokens with 1231 phrases; found: 1094 phrases; correct: 850.\n",
            "accuracy:  73.79%; (non-O)\n",
            "accuracy:  94.36%; precision:  77.70%; recall:  69.05%; FB1:  73.12\n",
            "              LOC: precision:  87.16%; recall:  78.51%; FB1:  82.61  327\n",
            "             MISC: precision:  79.29%; recall:  57.81%; FB1:  66.87  140\n",
            "              ORG: precision:  66.91%; recall:  59.28%; FB1:  62.87  272\n",
            "              PER: precision:  76.62%; recall:  73.71%; FB1:  75.14  355\n",
            "(77.6965265082267, 69.04955320877335, 73.11827956989248)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: Company <unk> <unk> <unk> Inc\n",
            "TRUE: O I-MISC I-ORG I-ORG I-ORG\n",
            "PRED: O O O I-ORG I-ORG\n",
            "SENT: BUENOS AIRES 1996-08-26\n",
            "TRUE: I-LOC I-LOC O\n",
            "PRED: I-LOC I-LOC O\n",
            "SENT: The <unk> <unk> of Nigeria ( <unk> ) quoted police spokesman <unk> <unk> as saying the six were killed on Wednesday .\n",
            "TRUE: O I-ORG I-ORG I-ORG I-ORG O I-ORG O O O O I-PER I-PER O O O O O O O O O\n",
            "PRED: O O O O I-LOC O I-ORG O O O O I-PER I-PER O O O O O O O O O\n",
            "SENT: <unk> kills 0000 at <unk> <unk> <unk> .\n",
            "TRUE: O O O O I-MISC O O O\n",
            "PRED: O O O O O O O O\n",
            "SENT: <unk> opposition newspaper <unk> <unk> .\n",
            "TRUE: I-MISC O O O O O\n",
            "PRED: I-ORG O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618]\n",
            "--- EPOCH 15 ---\n",
            "Avg loss over last 500 updates: 0.6704555339813233\n",
            "Avg loss over last 500 updates: 0.7189857265949249\n",
            "Avg loss over last 500 updates: 0.7003376245498657\n",
            "Avg loss over last 500 updates: 0.6747384071350098\n",
            "Avg loss over last 500 updates: 0.59068141746521\n",
            "Avg loss over last 500 updates: 0.7344006948471069\n",
            "Avg evaluation loss: 3.095687638223171\n",
            "processed 11170 tokens with 1231 phrases; found: 1136 phrases; correct: 863.\n",
            "accuracy:  75.66%; (non-O)\n",
            "accuracy:  94.31%; precision:  75.97%; recall:  70.11%; FB1:  72.92\n",
            "              LOC: precision:  87.24%; recall:  80.99%; FB1:  84.00  337\n",
            "             MISC: precision:  78.17%; recall:  57.81%; FB1:  66.47  142\n",
            "              ORG: precision:  68.21%; recall:  62.21%; FB1:  65.08  280\n",
            "              PER: precision:  70.82%; recall:  72.36%; FB1:  71.58  377\n",
            "(75.96830985915493, 70.10560519902518, 72.91930713983945)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: Bowling : <unk> <unk> ( <unk> ) , Kennedy <unk> ( <unk> <unk> ) ,\n",
            "TRUE: O O I-PER O O O O O I-PER O O O O O O\n",
            "PRED: O O I-PER I-PER O O O O I-PER I-PER O I-PER O O O\n",
            "SENT: One officer and a <unk> killed in <unk> clash between two groups of soldiers near <unk> in <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O I-LOC O I-LOC O\n",
            "PRED: O O O O O O O O O O O O O O O O O I-LOC O\n",
            "SENT: Canada 's <unk> <unk> finished second in his Williams\n",
            "TRUE: I-LOC O I-PER I-PER O O O O I-ORG\n",
            "PRED: I-LOC O O I-PER O O O O O\n",
            "SENT: The Greek <unk> party 's executive bureau gave the <unk> light to Prime Minister <unk> <unk> to call <unk> elections , its general secretary <unk> <unk> told reporters .\n",
            "TRUE: O I-MISC O O O O O O O O O O O O I-PER I-PER O O O O O O O O I-PER I-PER O O O\n",
            "PRED: O I-MISC O O O O O O O O O O O O I-PER I-PER O O O O O O O O I-PER I-PER O O O\n",
            "SENT: <unk> <unk> of Finland , driving a <unk> , on Monday won the 1,000 Lakes Rally , sixth round of the world championship .\n",
            "TRUE: I-PER I-PER O I-LOC O O O I-ORG O O O O O I-MISC I-MISC I-MISC O O O O O O O O\n",
            "PRED: O O O I-LOC O O O O O O O O O I-MISC I-MISC I-MISC O O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697]\n",
            "--- EPOCH 16 ---\n",
            "Avg loss over last 500 updates: 0.6622566969394684\n",
            "Avg loss over last 500 updates: 0.7511019308567047\n",
            "Avg loss over last 500 updates: 0.6439830923080444\n",
            "Avg loss over last 500 updates: 0.6598027319908142\n",
            "Avg loss over last 500 updates: 0.6560569832324982\n",
            "Avg loss over last 500 updates: 0.7165429153442383\n",
            "Avg evaluation loss: 3.2686160454154014\n",
            "processed 11170 tokens with 1231 phrases; found: 1191 phrases; correct: 872.\n",
            "accuracy:  76.54%; (non-O)\n",
            "accuracy:  94.14%; precision:  73.22%; recall:  70.84%; FB1:  72.01\n",
            "              LOC: precision:  85.10%; recall:  81.82%; FB1:  83.43  349\n",
            "             MISC: precision:  75.71%; recall:  55.21%; FB1:  63.86  140\n",
            "              ORG: precision:  65.31%; recall:  62.54%; FB1:  63.89  294\n",
            "              PER: precision:  67.89%; recall:  75.07%; FB1:  71.30  408\n",
            "(73.21578505457599, 70.83671811535336, 72.00660611065234)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: <unk> 1996-12-06\n",
            "TRUE: I-LOC O\n",
            "PRED: I-LOC O\n",
            "SENT: <unk>\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: -DOCSTART-\n",
            "TRUE: O\n",
            "PRED: O\n",
            "SENT: <unk> kills Polish <unk> <unk> , <unk> husband .\n",
            "TRUE: O O I-MISC O O O O O O\n",
            "PRED: O O I-MISC O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072]\n",
            "--- EPOCH 17 ---\n",
            "Avg loss over last 500 updates: 0.6711494207382203\n",
            "Avg loss over last 500 updates: 0.7537188696861267\n",
            "Avg loss over last 500 updates: 0.6161539645195008\n",
            "Avg loss over last 500 updates: 0.7361184153556823\n",
            "Avg loss over last 500 updates: 0.5150133628845215\n",
            "Avg loss over last 500 updates: 0.5733520255088806\n",
            "Avg evaluation loss: 3.505631369948387\n",
            "processed 11170 tokens with 1231 phrases; found: 1130 phrases; correct: 867.\n",
            "accuracy:  74.78%; (non-O)\n",
            "accuracy:  94.55%; precision:  76.73%; recall:  70.43%; FB1:  73.44\n",
            "              LOC: precision:  86.57%; recall:  83.47%; FB1:  84.99  350\n",
            "             MISC: precision:  80.15%; recall:  56.77%; FB1:  66.46  136\n",
            "              ORG: precision:  71.54%; recall:  58.96%; FB1:  64.64  253\n",
            "              PER: precision:  70.08%; recall:  74.25%; FB1:  72.11  391\n",
            "(76.72566371681417, 70.43054427294882, 73.44345616264295)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: 3. <unk> 0000\n",
            "TRUE: O I-PER O\n",
            "PRED: O I-PER O\n",
            "SENT: 0000 <unk>\n",
            "TRUE: O O\n",
            "PRED: O O\n",
            "SENT: Men 's <unk> <unk> <unk> Owens <unk> race\n",
            "TRUE: O O O O I-PER I-PER O O\n",
            "PRED: O O O O I-PER I-PER O O\n",
            "SENT: Some <unk> were delayed as <unk> <unk> <unk> lines .\n",
            "TRUE: O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O\n",
            "SENT: One officer and a <unk> killed in <unk> clash between two groups of soldiers near <unk> in <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O I-LOC O I-LOC O\n",
            "PRED: O O O O O O O O O O O O O O O O O I-LOC O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072, 0.6412561730334633]\n",
            "--- EPOCH 18 ---\n",
            "Avg loss over last 500 updates: 0.6370611276626587\n",
            "Avg loss over last 500 updates: 0.7228115391731262\n",
            "Avg loss over last 500 updates: 0.6360566153526306\n",
            "Avg loss over last 500 updates: 0.5790898904800416\n",
            "Avg loss over last 500 updates: 0.5471290264129639\n",
            "Avg loss over last 500 updates: 0.5576303453445435\n",
            "Avg evaluation loss: 3.521727678030729\n",
            "processed 11170 tokens with 1231 phrases; found: 1132 phrases; correct: 863.\n",
            "accuracy:  74.28%; (non-O)\n",
            "accuracy:  94.33%; precision:  76.24%; recall:  70.11%; FB1:  73.04\n",
            "              LOC: precision:  87.80%; recall:  81.27%; FB1:  84.41  336\n",
            "             MISC: precision:  80.88%; recall:  57.29%; FB1:  67.07  136\n",
            "              ORG: precision:  69.03%; recall:  60.26%; FB1:  64.35  268\n",
            "              PER: precision:  69.64%; recall:  73.98%; FB1:  71.75  392\n",
            "(76.23674911660777, 70.10560519902518, 73.04274227676683)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: His crucial <unk> partnership with fellow <unk> <unk> Law , who scored 0000 , added 0000 off 0000 balls .\n",
            "TRUE: O O O O O O O I-PER I-PER O O O O O O O O O O O\n",
            "PRED: O O O O O O O I-PER I-PER O O O O O O O O O O O\n",
            "SENT: Dumbarton 0000 <unk> 0000\n",
            "TRUE: I-ORG O I-ORG O\n",
            "PRED: I-ORG O I-ORG O\n",
            "SENT: 14. Alexandra <unk> ( <unk>\n",
            "TRUE: O I-PER I-PER O O\n",
            "PRED: O I-PER I-PER O O\n",
            "SENT: Gujral said India had national security <unk> that made it <unk> for New <unk> to sign the <unk> .\n",
            "TRUE: I-PER O I-LOC O O O O O O O O O I-LOC I-LOC O O O I-MISC O\n",
            "PRED: I-PER O I-LOC O O O O O O O O O I-LOC O O O O O O\n",
            "SENT: <unk> 0000 0000 0000 0000 0000 0000 0000\n",
            "TRUE: I-ORG O O O O O O O\n",
            "PRED: I-ORG O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295, 73.04274227676683]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072, 0.6412561730334633, 0.603243356977987]\n",
            "--- EPOCH 19 ---\n",
            "Avg loss over last 500 updates: 0.5262013514041901\n",
            "Avg loss over last 500 updates: 0.6730352051258087\n",
            "Avg loss over last 500 updates: 0.568339765548706\n",
            "Avg loss over last 500 updates: 0.5067094912528992\n",
            "Avg loss over last 500 updates: 0.46600664710998535\n",
            "Avg loss over last 500 updates: 0.5581764268875122\n",
            "Avg evaluation loss: 3.4788727389276026\n",
            "processed 11170 tokens with 1231 phrases; found: 1139 phrases; correct: 871.\n",
            "accuracy:  75.94%; (non-O)\n",
            "accuracy:  94.66%; precision:  76.47%; recall:  70.76%; FB1:  73.50\n",
            "              LOC: precision:  85.50%; recall:  79.61%; FB1:  82.45  338\n",
            "             MISC: precision:  76.87%; recall:  58.85%; FB1:  66.67  147\n",
            "              ORG: precision:  71.59%; recall:  61.56%; FB1:  66.20  264\n",
            "              PER: precision:  71.79%; recall:  75.88%; FB1:  73.78  390\n",
            "(76.47058823529412, 70.75548334687247, 73.50210970464134)\n",
            "\n",
            "5 random evaluation samples:\n",
            "SENT: But once the <unk> Stich got on the court , he <unk> his <unk> on trying to win the year 's last Grand Slam .\n",
            "TRUE: O O O O I-PER O O O O O O O O O O O O O O O O O I-MISC I-MISC O\n",
            "PRED: O O O O I-PER O O O O O O O O O O O O O O O O O I-MISC I-MISC O\n",
            "SENT: The Brazilian 's <unk> effort was enough to <unk> Real a point from a <unk> 1-1 draw at fellow title contenders <unk> <unk> .\n",
            "TRUE: O I-MISC O O O O O O O I-ORG O O O O O O O O O O O I-ORG I-ORG O\n",
            "PRED: O I-MISC O O O O O O O O O O O O O O O O O O O O O O\n",
            "SENT: <unk> health began to <unk> in 0000 when she was <unk> with a heart <unk> .\n",
            "TRUE: O O O O O O O O O O O O O O O O\n",
            "PRED: O O O O O O O O O O O O O O O O\n",
            "SENT: <unk> 1996-08-29\n",
            "TRUE: I-LOC O\n",
            "PRED: I-LOC O\n",
            "SENT: <unk> 0000 0000 0000 0000 0000 0000 0000\n",
            "TRUE: I-ORG O O O O O O O\n",
            "PRED: I-ORG O O O O O O O\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295, 73.04274227676683, 73.50210970464134]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072, 0.6412561730334633, 0.603243356977987, 0.5534306035404317]\n",
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295, 73.04274227676683, 73.50210970464134]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKfh_2TwA6UD",
        "outputId": "202a33bb-173e-49ab-fca6-f06ff1aac404"
      },
      "source": [
        "print(F1_scores)\n",
        "print(losses)\n",
        "print(l)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295, 73.04274227676683, 73.50210970464134]\n",
            "[6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072, 0.6412561730334633, 0.603243356977987, 0.5534306035404317]\n",
            "[4.526355362534523, 3.409770675301552, 2.974259833395481, 2.811852522492409, 2.7391921460628508, 2.7203063863515853, 2.727693136036396, 2.7438551032543184, 2.7880976888537408, 2.8332940067350862, 3.0228522665798665, 3.168568170815706, 3.069324896633625, 3.238306036889553, 3.282989797592163, 3.095687638223171, 3.2686160454154014, 3.505631369948387, 3.521727678030729, 3.4788727389276026]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "dtkVRbkjRPvF",
        "outputId": "340fcd0c-4a3b-4e09-9164-9ca7e2bde6e3"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "x, y=[], []\n",
        "count=0 \n",
        "# x axis values\n",
        "for i in (losses):\n",
        "    count+=1\n",
        "    y.append(count)\n",
        "    x.append(i)\n",
        "print(y,x)\n",
        "\n",
        "  \n",
        "# plotting the points \n",
        "plt.plot(y,x, color='red')\n",
        "plt.plot(y,l, color='green')\n",
        "  \n",
        "# naming the x axis\n",
        "plt.xlabel('epochs')\n",
        "# naming the y axis\n",
        "plt.ylabel('losses')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title(\"Average loss per epoch\")\n",
        "red_patch = mpatches.Patch(color='red', label='Training Loss')\n",
        "\n",
        "g_patch = mpatches.Patch(color='green', label='Validation Loss')\n",
        "plt.legend(handles=[g_patch, red_patch])\n",
        "# function to show the plot\n",
        "plt.show()\n",
        "#-----------------------------------------------------------------------\n",
        "x, y=[], []\n",
        "count=0 \n",
        "# x axis values\n",
        "for i in (F1_scores):\n",
        "    count+=1\n",
        "    y.append(count)\n",
        "    x.append(i)\n",
        "print(y,x)\n",
        "\n",
        "  \n",
        "# plotting the points \n",
        "plt.plot(y,x)\n",
        "  \n",
        "# naming the x axis\n",
        "plt.xlabel('epochs')\n",
        "# naming the y axis\n",
        "plt.ylabel('FB1 score')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title(\"FB1score\")\n",
        "  \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] [6.6432928626997425, 3.624143900020778, 2.5993108559770195, 2.0674385428080084, 1.7451094610997808, 1.4974315601482726, 1.346749971205728, 1.1971876372370804, 1.1051560028254637, 0.9938622546823401, 0.9214869599593313, 0.8922975646473511, 0.8214971598477392, 0.7831100260305126, 0.7325638329773618, 0.6775784674443697, 0.6774572501057072, 0.6412561730334633, 0.603243356977987, 0.5534306035404317]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] [46.07543323139653, 58.994833255049315, 65.75591985428049, 66.4565511031067, 69.3498452012384, 68.6842105263158, 70.66025360734587, 72.5841714035855, 72.08695652173913, 71.35593220338983, 72.6261762189906, 72.40484429065742, 72.59574468085106, 72.64957264957265, 73.11827956989248, 72.91930713983945, 72.00660611065234, 73.44345616264295, 73.04274227676683, 73.50210970464134]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfSImZWOvQB_"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}